{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nS7xB7GBmtsF"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "import os\n",
        "\n",
        "\n",
        "# Data Preprocessing before training\n",
        "\n",
        "with open(\"/content/urlhaus.abuse.ch.txt\", \"r\") as f:\n",
        "    urls = [line.strip() for line in f if not line.startswith(\"#\") and line.strip()]\n",
        "\n",
        "df = pd.DataFrame(urls, columns=['url'])\n",
        "\n",
        "# Function to extract components of urls, as BERT trains on text\n",
        "def extract_url_components(url):\n",
        "    parsed = urlparse(url)\n",
        "    hostname = parsed.hostname\n",
        "    ip_pattern = r\"^\\d{1,3}(\\.\\d{1,3}){3}$\"\n",
        "    is_ip = bool(re.match(ip_pattern, hostname)) if hostname else False\n",
        "    domain = hostname if not is_ip else hostname\n",
        "    subdomain = \"\" if is_ip or not hostname else '.'.join(hostname.split('.')[:-2]) if hostname.count('.') > 1 else ''\n",
        "    path = parsed.path\n",
        "    file_ext = os.path.splitext(parsed.path)[1] if '.' in os.path.basename(parsed.path) else ''\n",
        "    query_params = parsed.query\n",
        "    return pd.Series([domain, subdomain, path, file_ext, query_params, is_ip])\n",
        "\n",
        "\n",
        "df[['Domain', 'Subdomain', 'Path', 'File Extension', 'Query Parameters', 'Contains_IP']] = df['url'].apply(extract_url_components)\n",
        "df['label'] = [1] * len(df['url'])\n",
        "\n",
        "\n",
        "# Data for training from these files\n",
        "phis = pd.read_csv('/content/phishing-urls.csv')\n",
        "phis['label'] = [1] * len(phis['Domain'])\n",
        "phis['url'] = phis['Domain'] + phis['Path']\n",
        "phis_df = phis[['url','label']]\n",
        "leg = pd.read_csv('/content/legitimate-urls.csv')\n",
        "leg['label'] = [0] * len(leg['Domain'])\n",
        "leg['url'] = leg['Domain'] + leg['Path']\n",
        "leg_df = leg[['url','label']]\n",
        "\n",
        "\n",
        "\n",
        "# GEtting data into the right format\n",
        "df_combined = pd.concat([phis_df, leg_df], ignore_index=True)\n",
        "df_combined['url'] = df_combined['url'].astype(str).fillna('')\n",
        "df_combined[['Domain', 'Subdomain', 'Path', 'File Extension', 'Query Parameters', 'Contains_IP']] = (\n",
        "    df_combined['url'].apply(extract_url_components).apply(pd.Series)\n",
        ")\n",
        "df_combined['label'] = df_combined['label'].astype(bool)\n",
        "\n",
        "df_final_last = pd.concat([df, df_combined], ignore_index=True)\n",
        "df_final_last['label'] = df_final_last['label'].astype(int)\n",
        "\n"
      ],
      "metadata": {
        "id": "zv90Kn2rm_nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Training Model\n",
        "import pandas as pd\n",
        "import torch\n",
        "import time\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Splitting the data into training and testing sets for validation later\n",
        "train_df, test_df = train_test_split(df_final_last, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Custom Dataset Class to reuse code\n",
        "class URLDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Extract features\n",
        "        url = self.data.iloc[index]['url']\n",
        "        domain = self.data.iloc[index]['Domain']\n",
        "        subdomain = self.data.iloc[index]['Subdomain']\n",
        "        path = self.data.iloc[index]['Path']\n",
        "        file_extension = self.data.iloc[index]['File Extension']\n",
        "        query_params = self.data.iloc[index]['Query Parameters']\n",
        "        contains_ip = self.data.iloc[index]['Contains_IP']\n",
        "\n",
        "        # Combine features into a single string\n",
        "        combined_text = (\n",
        "            f\"URL: {url} Domain: {domain} Subdomain: {subdomain} \"\n",
        "            f\"Path: {path} File Extension: {file_extension} \"\n",
        "            f\"Query Parameters: {query_params} Contains IP: {contains_ip}\"\n",
        "        )\n",
        "\n",
        "        # Tokenize the combined text\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            combined_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        # Get the label\n",
        "        label = self.data.iloc[index]['label']\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Making Dataset\n",
        "max_len = 128\n",
        "train_dataset = URLDataset(train_df, tokenizer, max_len)\n",
        "test_dataset = URLDataset(test_df, tokenizer, max_len)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=16)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=16)\n"
      ],
      "metadata": {
        "id": "7lF6C5tr3rKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Loading the BERT model for sequence classification\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    # Used GPU in training my model , so always use GPU to run it\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "epochs = 2   # Number of epochs can be changed , but due to computational constraints kept it as 2\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_dataloader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f'Epoch {epoch + 1}/{epochs}, Batch {batch_idx}/{len(train_dataloader)}, Loss: {loss.item()}')\n",
        "\n",
        "    # Calculate average training loss\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    end_time = time.time()\n",
        "    #print(f'Epoch {epoch + 1}/{epochs} - Training Loss: {avg_train_loss}, Time: {end_time - start_time} seconds')       Comment can be removed for more real time tracking of the progress\n",
        "\n",
        "# Evaluating the model\n",
        "# Initializing the lists\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "precision = precision_score(all_labels, all_preds)\n",
        "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Confusion Matrix:\\n{conf_matrix}')"
      ],
      "metadata": {
        "id": "aQLSI3ny3xNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_preds, test_labels = [], []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    preds = torch.argmax(outputs.logits, dim=1)\n",
        "    test_preds.extend(preds.cpu().numpy())\n",
        "    test_labels.extend(batch['label'].cpu().numpy())\n",
        "\n",
        "print(f\"Accuracy: {accuracy_score(test_labels, test_preds)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2KI1yHh9hh4",
        "outputId": "6a83a166-02a4-4925-b4de-3710db900f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "toFhTw6C-bNe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}